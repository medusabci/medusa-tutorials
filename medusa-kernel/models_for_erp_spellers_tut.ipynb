{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the creation of models for ERP-based spellers tutorial!\n",
    "\n",
    "Medusa is a framework designed for scientists and developers who investigate\n",
    "novel signal processing algorithms, reducing the development and testing time\n",
    "in real experiments. This includes not only the implementation of cutting-edge\n",
    "signal processing methods, but also high level functionalities to assure the\n",
    "persistence and reproducibility of the algorithms created within the framework.\n",
    "One of they key features that makes Medusa so powerful is its ability to\n",
    "implement and share standalone algorithms out of the box compatible with\n",
    "Medusa applications.\n",
    "\n",
    "In this notebook you will learn:\n",
    "- How to create a custom model for ERP-based spellers\n",
    "- Save the algorithm\n",
    "- Use the algorithm in Medusa platform\n",
    "\n",
    "Before this tutorial, make sure you have checked:\n",
    "- [Algorithm creation tutorial](algorithms_tut_basic.ipynb)\n",
    "- [Overview of erp_spellers module](erp_spellers_tut.ipynb)\n",
    "\n",
    "Do not forget to check the documentation if you do not understand something!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the world of brain-computer interfaces, novel algorithms arise every day to\n",
    "improve these systems. However, most of these methods are not tested in online\n",
    "experiments due to the technical complexity and time required to develop full\n",
    "stack BCIs, putting in doubt their real usefulness. With Medusa, you can\n",
    "focus on the development of new algorithms because, by following some simple\n",
    "rules, you can implement a standalone algorithm to decode brain signals in\n",
    "real time and put it in production within minutes! All of this, assuring\n",
    "interoperability with existing frameworks such as sklearn, mne, etc. Ready?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the modules that will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# Built-in imports\n",
    "import glob\n",
    "\n",
    "# External imports\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Medusa imports\n",
    "from medusa import components\n",
    "from medusa import meeg\n",
    "from medusa.bci import erp_spellers\n",
    "\n",
    "\n",
    "def print_acc_per_seq(acc_per_seq):\n",
    "    table_cmd_acc_per_seq = ['Command decoding acc']\n",
    "    cmd_acc_per_seq = np.char.mod('%.2f', acc_per_seq*100).astype(str).tolist()\n",
    "    table_cmd_acc_per_seq += cmd_acc_per_seq\n",
    "    headers = [''] + list(range(1, 16))\n",
    "    print('\\nTrain accuracy per number of sequences of stimulation:\\n')\n",
    "    print(tabulate([table_cmd_acc_per_seq], headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "As strong supporters of open science, we have released and adapted some\n",
    "valuable datasets that can be very useful for researchers and practitioners.\n",
    "These datasets can be downloaded manually from www.medusa.com/datasets/ or\n",
    "using a simple API. In this case, we will use the API. Run the following cell\n",
    "to download the GIB-UVa ERP dataset [1].\n",
    "\n",
    "Each file is an instance of medusa.data_structures.Recording. This class\n",
    "contains the information of the performed experiment, and the recorded\n",
    "biosignals. In this case, the recordings contain an instance of\n",
    "medusa.components.ERPSpellerData, which is the default class for\n",
    "ERPBasedSpellers. Additionally, all recordings contain a medusa.meeg.EEG\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Download dataset\n",
    "# dataset_folder = os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% cd\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cha_set = meeg.EEGChannelSet()\n",
    "cha_set.set_standard_channels(l_cha=['Fz', 'Cz', 'Pz', 'Oz'])\n",
    "dataset = erp_spellers.ERPSpellerDataset(channel_set=cha_set,\n",
    "                                         biosignal_att_key='eeg',\n",
    "                                         experiment_att_key='erpspellerdata',\n",
    "                                         experiment_mode='train')\n",
    "\n",
    "print('OK!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% cd\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add recordings to the dataset\n",
    "\n",
    "Now, we have to add the recordings to the dataset. With this purpose, we read\n",
    "the files that were downloaded and use the function add_recordings of our\n",
    "dataset. Note that this function admits instances of medusa.components.Recording\n",
    "or a list of paths. For convenience, we will use the second option in this case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "file_pattern = '*.rcp.bson'\n",
    "files = glob.glob('%s/%s' % (folder, file_pattern))\n",
    "dataset.add_recordings(files)\n",
    "\n",
    "print('OK!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% cd\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The next step is to instantiate the methods that will compose the algorithm,\n",
    "but take into account that only methods that inherit from\n",
    "medusa.components.ProcessingMethod can be added to the\n",
    "Algorithm class. Medusa framework includes a wide variety of signal processing\n",
    "methods ready to use. Nevertheless, function and class wrappers have also been\n",
    "designed to assure full interoperability with external packages.\n",
    "\n",
    "To show these functionalities, we will implement a custom algorithm based on a\n",
    "support vector machine (SVM) using the sklearn package. The algorithm will\n",
    "have the following stages:\n",
    "1. **Preprocessing:** frequency filtering using an IIR filter with order=5 and\n",
    "cutoff frequences in (0.5, 10) Hz and spatial filtering using common average\n",
    "reference (CAR).\n",
    "2. **Feature extraction:** EEG epochs from (0, 1000) ms after each stimulus\n",
    "onset, baseline normalization (-250, 0) ms and downsampling to 20 Hz\n",
    "3. **Feature classification:** SVM classifier using the implementation of\n",
    "sklearn wrapped with ProcessingClassWrapper.\n",
    "4. **Command decoding:** additional data processing to decode the selected\n",
    "commands form predicted scores of EEG epochs.\n",
    "4. **Model assessment:** method to calculate the accuracy of the model as a\n",
    "function of the number of sequences of stimulation\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Preprocessing\n",
    "prep = erp_spellers.StandardPreprocessing()\n",
    "\n",
    "# 2. Feature extractor\n",
    "feat_ext = erp_spellers.StandardFeatureExtraction()\n",
    "\n",
    "# 3. Classifier. We must define the methods and output variables that will be\n",
    "# exposed to the algorithm. In this case, we will need fit and predict_proba.\n",
    "# See the sklearn documentation to learn more about this classifier.\n",
    "clf = components.ProcessingClassWrapper(\n",
    "    SVC(), fit=[], predict=['y_pred']\n",
    ")\n",
    "\n",
    "# 4. Command decoding function to decode the predicted command from epochs\n",
    "# scores\n",
    "cmd_decoding = components.ProcessingFuncWrapper(\n",
    "    erp_spellers.decode_commands,\n",
    "    outputs=['spell_result', 'spell_result_per_seq', 'scores']\n",
    ")\n",
    "\n",
    "# 5. Method to calculate the accuracy of the classifier per number of\n",
    "# sequences of stimulation\n",
    "model_assessment = components.ProcessingFuncWrapper(\n",
    "    erp_spellers.command_decoding_accuracy_per_seq,\n",
    "    outputs=['spell_acc_per_seq']\n",
    ")\n",
    "\n",
    "# Create algorithm instance and add the methods\n",
    "alg = erp_spellers.ERPSpellerModel()\n",
    "alg.add_method('prep', prep)\n",
    "alg.add_method('feat-ext', feat_ext)\n",
    "alg.add_method('clf', clf)\n",
    "alg.add_method('cmd-decoding', cmd_decoding)\n",
    "alg.add_method('assessment', model_assessment)\n",
    "\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define model pipelines\n",
    "\n",
    "Once the methods have been added to the algorithm, it's time to define the\n",
    "algorithm processing pipelines. Models based on ERPSpellerModel, which\n",
    "inherits from components.Algorithm, have to implement 2 pipelines: one to fit\n",
    "the algorithm from a dataset, and one to predict commands from EEG signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fit_dataset_pipeline():\n",
    "    pipe = components.Pipeline()\n",
    "    uid_0 = pipe.input(['dataset'])\n",
    "    uid_1 = pipe.add(\n",
    "        method_func_key='prep:fit_transform_dataset',\n",
    "        dataset=pipe.conn_to(uid_0, 'dataset')\n",
    "    )\n",
    "    uid_2 = pipe.add(\n",
    "        method_func_key='feat-ext:transform_dataset',\n",
    "        dataset=pipe.conn_to(uid_1, 'dataset'),\n",
    "    )\n",
    "    uid_3 = pipe.add(\n",
    "        method_func_key='clf:fit',\n",
    "        X=pipe.conn_to(uid_2, 'x'),\n",
    "        y=pipe.conn_to(uid_2, 'x_info',\n",
    "                       conn_exp=lambda x_info: x_info['erp_labels']\n",
    "        )\n",
    "    )\n",
    "    uid_4 = pipe.add(\n",
    "        method_func_key='clf:predict',\n",
    "        X=pipe.conn_to(uid_2, 'x')\n",
    "    )\n",
    "    uid_5 = pipe.add(\n",
    "        method_func_key='cmd-decoding:decode_commands',\n",
    "        scores=pipe.conn_to(uid_4, 'y_pred'),\n",
    "        paradigm_conf=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['paradigm_conf']),\n",
    "        run_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['run_idx']),\n",
    "        trial_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['trial_idx']),\n",
    "        matrix_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['matrix_idx']),\n",
    "        level_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['level_idx']),\n",
    "        unit_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['unit_idx']),\n",
    "        sequence_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['sequence_idx']),\n",
    "        group_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['group_idx']),\n",
    "        batch_idx=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['batch_idx']),\n",
    "    )\n",
    "    uid_6 = pipe.add(\n",
    "        method_func_key='assessment:command_decoding_accuracy_per_seq',\n",
    "        selected_commands_per_seq=pipe.conn_to(\n",
    "            uid_5, 'spell_result_per_seq'\n",
    "        ),\n",
    "        target_commands=pipe.conn_to(\n",
    "            uid_2, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['spell_target']\n",
    "        )\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "def predict_pipeline():\n",
    "    pipe = components.Pipeline()\n",
    "    uid_0 = pipe.input(['times', 'signal', 'fs', 'x_info'])\n",
    "    uid_1 = pipe.add(\n",
    "        method_func_key='prep:fit_transform_signal',\n",
    "        signal=pipe.conn_to(uid_0, 'signal'),\n",
    "        fs=pipe.conn_to(uid_0, 'fs')\n",
    "    )\n",
    "    uid_2 = pipe.add(\n",
    "        method_func_key='feat-ext:transform_signal',\n",
    "        times=pipe.conn_to(uid_0, 'times'),\n",
    "        signal=pipe.conn_to(uid_1, 'signal'),\n",
    "        fs=pipe.conn_to(uid_0, 'fs'),\n",
    "        onsets=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['onsets']\n",
    "        ),\n",
    "    )\n",
    "    uid_3 = pipe.add(\n",
    "        method_func_key='clf:predict',\n",
    "        X=pipe.conn_to(uid_2, 'x'),\n",
    "    )\n",
    "    uid_4 = pipe.add(\n",
    "        method_func_key='cmd-decoding:decode_commands',\n",
    "        scores=pipe.conn_to(uid_3, 'y_pred'),\n",
    "        paradigm_conf=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['paradigm_conf']),\n",
    "        run_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['run_idx']),\n",
    "        trial_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['trial_idx']),\n",
    "        matrix_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['matrix_idx']),\n",
    "        level_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['level_idx']),\n",
    "        unit_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['unit_idx']),\n",
    "        sequence_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['sequence_idx']),\n",
    "        group_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['group_idx']),\n",
    "        batch_idx=pipe.conn_to(\n",
    "            uid_0, 'x_info',\n",
    "            conn_exp=lambda x_info: x_info['batch_idx']),\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "alg.add_pipeline('fit_dataset', fit_dataset_pipeline())\n",
    "alg.add_pipeline('predict', predict_pipeline())\n",
    "\n",
    "print('OK!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% cd\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fit model\n",
    "\n",
    "Time to fit the model! call to function fit to execute fit-dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# Execute fit pipeline\n",
    "fit_res = alg.fit_dataset(dataset)\n",
    "print_acc_per_seq(fit_res[6]['res']['spell_acc_per_seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict commands\n",
    "\n",
    "Time to predict some commands simulating an online experiment! call function\n",
    "predict to execute predict pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get some signal\n",
    "rec = dataset.recordings[0]\n",
    "times = rec.eeg.times\n",
    "signal = rec.eeg.signal\n",
    "fs = rec.eeg.fs\n",
    "l_cha = rec.eeg.channel_set.l_cha\n",
    "x_info = {'onsets': rec.erpspellerdata.onsets,\n",
    "          'paradigm_conf': [rec.erpspellerdata.paradigm_conf],\n",
    "          'run_idx': np.zeros_like(rec.erpspellerdata.onsets),\n",
    "          'trial_idx': rec.erpspellerdata.trial_idx,\n",
    "          'matrix_idx': rec.erpspellerdata.matrix_idx,\n",
    "          'level_idx': rec.erpspellerdata.level_idx,\n",
    "          'unit_idx': rec.erpspellerdata.unit_idx,\n",
    "          'sequence_idx': rec.erpspellerdata.sequence_idx,\n",
    "          'group_idx': rec.erpspellerdata.group_idx,\n",
    "          'batch_idx': rec.erpspellerdata.batch_idx}\n",
    "\n",
    "# Execute predict pipeline\n",
    "predict_res = alg.predict(times, signal, fs, l_cha, x_info)\n",
    "\n",
    "print('\\nCommand decoding results:')\n",
    "print(rec.erpspellerdata.spell_target)\n",
    "print(predict_res[4]['res']['spell_result'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% cd\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence\n",
    "\n",
    "The Algorithm class includes persistence options to save the algorithm in\n",
    "the current state. Medusa uses dill as serialization tool and thus it has\n",
    "the same advantages and disadvantages of this tool.\n",
    "\n",
    "It is possible to come across classes that are not directly serializable with\n",
    "dill (e.g., keras models). In such cases, override methods 'to_pickleable_obj'\n",
    "and 'from_pickleable_obj' of class Processing method.\n",
    "\n",
    "Execute the next cell to save and load the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save algorithm\n",
    "alg.save('alg.pkl')\n",
    "\n",
    "# Load algorithm\n",
    "loaded_alg = erp_spellers.ERPSpellerModel.load('alg.pkl')\n",
    "\n",
    "# Predict with the loaded model\n",
    "predict_res = loaded_alg.predict(times, signal, fs, l_cha, x_info)\n",
    "print('\\nCommand decoding results:')\n",
    "print(rec.erpspellerdata.spell_target)\n",
    "print(predict_res[4]['res']['spell_result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone models\n",
    "\n",
    "Congratulations! The file alg.pkl in your working directory contains a\n",
    "standalone version of our algorithm. To load and use it in a different\n",
    "script or machine, use the following code:\n",
    "\n",
    "    >>> from medusa.bci import erp_spellers\n",
    "    >>> alg = erp_spellers.ERPSpellerModel.load('alg.pkl')\n",
    "\n",
    "Standalone algorithms are very useful for developers and scientists that design\n",
    "add-hoc algorithms for a certain problem, database, etc, and want to share them\n",
    "in an easy and quick way. Moreover, they are compatible with Medusa platform\n",
    "apps.\n",
    "\n",
    "Remember that only algorithms that contain methods accessible in the destination\n",
    "machine can be distributed as a single file. For example, our example\n",
    "can only be loaded in python environments which have sklearn installed. This\n",
    "shouldn't be a problem, even for the most complex examples, due to the huge\n",
    "amount of data processing packages available nowadays. Additionally, note\n",
    "that dill is able to deserialize functions from scratch.\n",
    "\n",
    "In the rare case that the available packages and dill functionalities don't suit\n",
    "your needs, you have 2 options to distribute your algorithm: distribute your\n",
    "code along with the algorithm file or create your own package in PyPI to easily\n",
    "install your methods in any computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "That's all for now! Now you have a comprehensive picture on how to create and\n",
    "use your own models for ERP-based spellers As you can see, you can build full\n",
    "signal processing pipelines in a very flexible and easy way with few code\n",
    "lines using Medusa!\n",
    "\n",
    "See you in the next tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}