{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome to the tutorial for motor imagery analysis with Medusa\n",
    "\n",
    "Motor imagery (MI) is a mental task where a subject imagines performing a movement without actually executing it. This task is commonly used in brain-computer interface (BCI) systems to control devices such as a cursor on a screen or a robotic arm. In this tutorial, we will use the Medusa library to analyze motor imagery data and train a classifier to distinguish between left and right hand movements. We will use the Common Spatial Pattern (CSP) algorithm to extract spatial features from the EEG signals and train a linear classifier to predict the movement class. The goal is to train a classifier that can distinguish between left and right hand movements based on the EEG signals.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load and visualize motor imagery data.\n",
    "- Create frequency and time-domain plots for two EEG channels.\n",
    "- Train a CSP (Common Spatial Pattern) model and evaluate its performance using k-fold cross-validation.\n",
    "- Plot the CSP filters to examine the learned spatial patterns.\n",
    "- Saving the model for future use\n",
    "- Loading the model and using it to predict new data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66f99152df2c7971"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our MI-based paradigm\n",
    "\n",
    "### Experiment briefing\n",
    "In this tutorial, we will work with pre-recorded data from a user participating in a c-VEP experiment using the __[`MI app`](https://www.medusabci.com/market/mi/)__ app from __[MEDUSA©](https://www.medusabci.com)__.\n",
    "\n",
    "The objective of the experiment was to control a ball that could move left and right. The user was instructed to imagine moving their left or right hand. There are 10 trials for each run with a duration of 6 seconds per trial. There are 10 runs in total with 4 corresponding to calibration and 6 to testing offering feedback.\n",
    "\n",
    "The EEG data were recorded from 16 active channels positioned at 'F3', 'FZ', 'F4', 'C3', 'CZ', 'C4', 'CPZ', 'P3', 'PZ', 'P4', 'PO7', 'POZ', 'PO8', 'OZ', 'I1',and 'I2' , with a sampling rate of 256 Hz."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90bc74195b418df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load calibration and test data\n",
    "First, if you are using Google Colab, execute the following cell to install `medusa-kernel` and clone the GitHub repository to obtain the EEG recordings.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "Important: execute the following cell **only** if you're using Google Collab!\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39464561f65cd4cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install medusa-kernel\n",
    "!git clone https://github.com/medusabci/medusa-tutorials.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "202bb40512723010"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to import the required libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c184131414808b6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# General imports\n",
    "from medusa.plots import mi_plots\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Medusa imports\n",
    "from medusa.components import Recording\n",
    "from medusa.bci.mi_paradigms import MIModelCSP, MIDataset\n",
    "\n",
    "print('> Import successful!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc3a2da106ccd90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Data\n",
    "We start by loading the dataset containing motor imagery recordings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59b6e3d480bbd708"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load calibration data and test data\n",
    "files_train = glob.glob(\"medusa-tutorials/mi/*train*.mi.bson\")\n",
    "files_test = glob.glob(\"medusa-tutorials/mi/*test*.mi.bson\")\n",
    "\n",
    "# Initialize the MIPlots object for plotting\n",
    "miplots = mi_plots.MIPlots()\n",
    "miplots.set_sizes(label_size=6, axes_size=5, line_width=1)\n",
    "\n",
    "# We will use one of the files for plotting relevant data\n",
    "miplots.set_dataset([files_test[0]])\n",
    "\n",
    "# Define EEG channels to plot\n",
    "ch1 = 'C3'\n",
    "ch2 = 'C4'\n",
    "\n",
    "# Define windows for baseline and epoch (relative to the onset of the cue)\n",
    "w_baseline = (-2000, 0)\n",
    "w_epoch = (500, 6000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "159a65c4e7c9c0cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "\n",
    "We are using the glob library to gather a list of files containing motor imagery recordings.\n",
    "The dataset is then set up using the MIPlots object, where we specify the \n",
    "channels C3 and C4 for plotting.\n",
    "Windows for the baseline and epoch are defined to extract relevant portions of the data around the motor imagery cue."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdebe36d12f9584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting Frequency-Domain Data\n",
    "Now, we plot the event-related desynchronization/synchronization (ERD/ERS) in the frequency domain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50d36be6bb6c3a55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prepare the figure\n",
    "fig = plt.figure(figsize=(5.5, 2.5), dpi=300, layout='constrained')\n",
    "gs = fig.add_gridspec(1, 2, wspace=0.1, hspace=0.1)\n",
    "\n",
    "# Frequency plots\n",
    "freq_axs_dict = list()\n",
    "freq_axs_dict.append({'freq': fig.add_subplot(gs[0:6, 0])})\n",
    "freq_axs_dict.append({'freq': fig.add_subplot(gs[0:6, 1])})\n",
    "\n",
    "freq_axs_dict = miplots.plot_erd_ers_freq(\n",
    "    ch_to_plot=(ch1, ch2),\n",
    "    axs_to_plot=freq_axs_dict,\n",
    "    f_lims=(5, 30),\n",
    "    f_sel=(8, 13),\n",
    "    t_trial_window=w_epoch\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d32b82eefa64a5b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "The plot_erd_ers_freq method is used to visualize frequency-specific power changes over time for channels C3 and C4."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42aaf850d5c02c7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the CSP Model\n",
    "In this section, we will configure and train a traditional Common Spatial Pattern (CSP) model using the motor imagery data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "469c7c26f3e02de6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define settings for the CSP model\n",
    "settings = {\n",
    "        'p_filt_cutoff': (8, 30),\n",
    "        'n_filters': 2,\n",
    "        'normalize_log_vars': False,\n",
    "        'w_epoch_t': w_epoch,\n",
    "        'baseline_mode': 'sliding',\n",
    "        'w_baseline_t': w_baseline,\n",
    "        'norm': 'z',\n",
    "        'target_fs': None,\n",
    "        'concatenate_channels': False,\n",
    "        'sliding_w_lims_t': w_epoch,\n",
    "        'sliding_t_step': 200,\n",
    "        'sliding_win_len': 2000\n",
    "    }\n",
    "\n",
    "# Initialize and configure the CSP model\n",
    "model = MIModelCSP()\n",
    "model.configure(**settings)\n",
    "model.build()\n",
    "\n",
    "# Load dataset and add recordings\n",
    "for i, _file in enumerate(files_train):\n",
    "    rec = Recording.load(_file)\n",
    "    if i == 0:\n",
    "        dataset = MIDataset(channel_set=rec.eeg.channel_set, fs=rec.eeg.fs, experiment_mode='train')\n",
    "    dataset.add_recordings(rec)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb5b5496871c257"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "\n",
    "The CSP model settings are defined, including parameters for frequency filtering and windowing.\n",
    "The dataset is then loaded, and recordings are added to the MIDataset object."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd7dc6b3e8023964"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross-Validation and Model Performance\n",
    "We now evaluate the model's performance using k-fold cross-validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f41dea579d681e97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "k_fold = 5\n",
    "fit_results = model.fit_dataset(dataset, k_fold=k_fold)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Estimating accuracy using {k_fold}-fold cross-validation\")\n",
    "for k in range(k_fold):\n",
    "    print(f\"Fold {k + 1}: {fit_results['k-fold'][k]['accuracy']*100:.2f}%\")\n",
    "print(f\"Estimated accuracy: {fit_results['accuracy'] * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a58a73cb2b64c8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "\n",
    "We use 5-fold cross-validation to estimate the model's accuracy.\n",
    "The accuracy for each fold is printed, along with the overall estimated \n",
    "accuracy from the model trained on all the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c976a15ca6271c0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting CSP Filters\n",
    "Finally, we visualize the spatial patterns learned by the CSP model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a6e6f3365f8b4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot the CSP model's filters\n",
    "fig_CSP = plt.figure(figsize=(3, 1.6), dpi=150, layout='constrained')\n",
    "model.methods[\"ext_method\"]['instance'].CSP.plot(\n",
    "    channel_set=model.channel_set,\n",
    "    plot_filters=False,\n",
    "    plot_patterns=True,\n",
    "    show=False, \n",
    "    figure=fig_CSP,\n",
    "    only_selected=True\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig_CSP.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaea08a638865bf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "\n",
    "The spatial filters (patterns) learned by the CSP model that are selected \n",
    "for classification are plotted.\n",
    "These patterns help us interpret the spatial distribution that the model \n",
    "extracts to distinguish between motor imagery of the left and right hand. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397eb67de564119e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save and load a model then predict dataset accuracy\n",
    "In this final part of the code, we will cover how to save the trained CSP model, reload it, and use it to make predictions on new data.\n",
    "\n",
    "#### Saving the Trained Model\n",
    "After training the CSP model, it's important to save it for future use. This step allows you to store the trained model parameters so that you can apply the model to new datasets or use it in your BCI experiments without retraining it every time. Here, we save the trained model using the save method:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e94e42dd3c831ab3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model_train.csp.mi.mdl')\n",
    "print('Model saved successfully!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b31404f2644cbfe6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading the Saved Model\n",
    "To use the saved model, we first need to load it from the saved file. This allows us to apply the previously trained model to a new dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38b4f53ac7563254"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_loaded = MIModelCSP.load('model_train.csp.mi.mdl')\n",
    "print('Model loaded successfully!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "796d8912206b141"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicting on New Data\n",
    "Now that we have loaded the model, we can use it to make predictions on a new dataset (files_test). This part simulates the process of applying the CSP model to unseen data to assess its performance in a real-world scenario."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8e0e2b8d966bdd6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the test data and prepare the dataset for prediction\n",
    "for i, _file in enumerate(files_test):\n",
    "    rec = Recording.load(_file)\n",
    "    if i == 0:\n",
    "        dataset_test = MIDataset(\n",
    "            channel_set=rec.eeg.channel_set,\n",
    "            fs=rec.eeg.fs,\n",
    "            experiment_mode='train'\n",
    "        )\n",
    "    dataset_test.add_recordings(rec)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "predict_results = model_loaded.predict_dataset(dataset_test)\n",
    "print(f\"\\n Model accuracy: {predict_results['accuracy'] * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e00afab5ae4cb9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "\n",
    "The predict_dataset method uses the loaded CSP model to predict labels for the new data. It computes the model’s accuracy based on how well the predictions match the true labels in the test dataset.\n",
    "The prediction accuracy is printed, giving us an indication of the model’s performance on unseen data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0cf3c9f3bd5d730"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
