{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the algorithm creation tutorial!\n",
    "\n",
    "The Algorithm class is a powerful tool that provides common ground for data\n",
    "processing pipelines as well as persistence functionalities. If you need to\n",
    "define and distribute standalone algorithms with full compatibility with\n",
    "medusa native methods as well as third party packages, this is your tutorial.\n",
    "This notebook will cover the main features, functions and classes involved in\n",
    "the definition of an algorithm through illustrative examples.\n",
    "\n",
    "In this notebook you will learn:\n",
    "- What is the ProcessingMethod class\n",
    "- Wrap functions and external classes in the ProcessingMethod class\n",
    "- Define a processing pipeline\n",
    "- Create an algorithm\n",
    "\n",
    "Do not forget to check the documentation if you do not understand something!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Medusa has been designed to facilitate the implementation of signal processing\n",
    "algorithms, meeting the needs of researchers and developers from different\n",
    "fields. This includes not only the implementation of cutting-edge ready-to-use\n",
    "signal processing methods, but also high level features to assure the\n",
    "persistence and reproducibility of the algorithms created within medusa. All of\n",
    "this, assuring interoperability with existing frameworks such as sklearn, mne,\n",
    "etc. The Algorithm class meets these requirements, providing a unique\n",
    "methodology to create, save and share complex data processing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the modules that will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "\n",
    "# External processing methods\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Medusa imports\n",
    "from medusa import components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, we will create some data, simulating a set of features extracted from a\n",
    "biosignal recording with the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: \n",
      "\tX shape: (200, 5)\n",
      "\ty shape: (200,)\n",
      "Test set: \n",
      "\tX shape: (10, 5)\n",
      "\ty shape: (10,)\n",
      "\n",
      "Test labels: [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "x_train = np.random.rand(1000).reshape((200, 5))\n",
    "x_train[:100, :] = x_train[:100, :] + 1\n",
    "x_train[100:, :] = x_train[100:, :] - 1\n",
    "\n",
    "y_train = np.zeros((200,))\n",
    "y_train[:100] = 1\n",
    "\n",
    "# Create test dataset\n",
    "x_test = np.random.rand(50).reshape((10, 5))\n",
    "x_test[:5, :] = x_test[:5, :] + 1\n",
    "x_test[5:, :] = x_test[5:, :] - 1\n",
    "\n",
    "y_test = np.zeros((10,))\n",
    "y_test[:5] = 1\n",
    "\n",
    "print('Train set: ')\n",
    "print('\\tX shape: %s' % str(x_train.shape))\n",
    "print('\\ty shape: %s' % str(y_train.shape))\n",
    "\n",
    "print('Test set: ')\n",
    "print('\\tX shape: %s' % str(x_test.shape))\n",
    "print('\\ty shape: %s' % str(y_test.shape))\n",
    "\n",
    "print()\n",
    "print('Test labels: %s' % str(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing methods\n",
    "\n",
    "The next step is to instantiate the methods that will compose the algorithm.\n",
    "Only methods that inherit from medusa.components.ProcessingMethod can be added\n",
    "to the Algorithm class. Medusa framework includes a wide variety of signal\n",
    "processing methods ready to use. Nevertheless, function and class wrappers\n",
    "have also been designed to assure full interoperability with external packages.\n",
    "\n",
    "To show these functionalities, we will implement a simple algorithm based on a\n",
    "linear discriminant analysis (LDA) using the sklearn package. First, we will\n",
    "implement a function to scale the input data, just to show how to use the class\n",
    "ProcessingFuncWrapper. Afterwards, we will use the ProcessingClassWrapper to\n",
    "wrap the LDA class from sklearn in ProcessingMethod. In practice, you could\n",
    "also create your own wrapping class inheriting from ProcessinMethod, but this\n",
    "would prevent the distribution of the algorithm in a standalone fashion. In\n",
    "that case, you would need to distribute the code of the class along with the\n",
    "algorithm file.\n",
    "\n",
    "An important issue is the definition of the functions that will be exposed to\n",
    "the algorithm for each processing method. Class Algorithm needs to know which\n",
    "functions of each method can be executed, and what are the outputs. We will have\n",
    "to define these parameters in the constructors of ProcessingFuncWrapper,\n",
    "ProcessingClassWrapper and ProcessingMethod. Be careful, these definitions will\n",
    "be used later to compile the processing pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "# 1. Data scaler\n",
    "def scale(x, n):\n",
    "    return n * x\n",
    "scaler = components. ProcessingFuncWrapper(scale, outputs=['x'])\n",
    "\n",
    "# 2. LDA classifier from sklearn using ProcessingClassWrapper\n",
    "lda = components.ProcessingClassWrapper(LinearDiscriminantAnalysis(),\n",
    "                                        fit=[], predict=['y'])\n",
    "\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Instantiate algorithm\n",
    "\n",
    "Once the methods have been instantiated, we will create the algorithm and add\n",
    "them, specifying a unique id for each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "# Algorithm\n",
    "alg = components.Algorithm()\n",
    "\n",
    "# Add methods\n",
    "alg.add_method('scaler', scaler)\n",
    "alg.add_method('lda', lda)\n",
    "\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the processing pipelines\n",
    "\n",
    "An algorithm can have several processing pipelines that can be executed\n",
    "independently. Some usage examples are the fit and apply stages of the\n",
    "methods, or the splitting of the algorithm in several independent stages\n",
    "(e.g., preprocessing, feature extraction, feature selection and feature\n",
    "classification). Therefore, the Algorithm class provides high flexibility to\n",
    "meet the needs of researchers and developers. Remember that the pipeline\n",
    "doesn't check the consistency of the connections. This will be done by the\n",
    "algorithm, which compiles the pipeline when add_pipeline function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train pipe\n",
    "train_pipe = components.Pipeline()\n",
    "uid_0 = train_pipe.input(['x', 'y'])\n",
    "uid_1 = train_pipe.add(method_func_key='scaler:scale',\n",
    "                       x=train_pipe.conn_to(uid_0, 'x'), n=2)\n",
    "uid_2 = train_pipe.add(method_func_key='lda:fit',\n",
    "                       X=train_pipe.conn_to(uid_0, 'x'),\n",
    "                       y=train_pipe.conn_to(uid_0, 'y'))\n",
    "# Test pipe\n",
    "test_pipe = components.Pipeline()\n",
    "uid_0 = test_pipe.input(['x'])\n",
    "uid_1 = test_pipe.add(method_func_key='scaler:scale',\n",
    "                      x=test_pipe.conn_to(uid_0, 'x'), n=2)\n",
    "uid_2 = test_pipe.add(method_func_key='lda:predict',\n",
    "                      X=test_pipe.conn_to(uid_1, 'x'))\n",
    "\n",
    "\n",
    "# Add pipelines\n",
    "alg.add_pipeline('train', train_pipe)\n",
    "alg.add_pipeline('test', test_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Execute pipelines\n",
    "\n",
    "Now, we will execute the pipelines. To do this, just call to exec_pipeline\n",
    "method of Algorithm, providing the correct input keys and values. Remember\n",
    "that the algorithm maintains the state between executions, so we can fit and\n",
    "test the implemented classifiers.\n",
    "\n",
    "Function exec_pipeline returns an ordered dict with the results of each step\n",
    "of the pipeline. Each position is a dict with the method-function key, their\n",
    "results assigned to each of the exposed outputs, and a simple performance\n",
    "analysis that, for the moment, only measures the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time lda:fit: 0.0024 s\n",
      "Result of lda_predict: [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Execute pipelines\n",
    "train_res = alg.exec_pipeline('train', x=x_train, y=y_train)\n",
    "test_res = alg.exec_pipeline('test', x=x_test)\n",
    "\n",
    "print()\n",
    "print('Execution time lda:fit: %.4f s' % train_res[2]['perf']['run_time'])\n",
    "print('Result of lda_predict: %s' % str(test_res[2]['res']['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence\n",
    "\n",
    "The Algorithm class includes persistence options to save the algorithm in\n",
    "the current state. Medusa uses dill as serialization tool and thus it has\n",
    "the same advantages and disadvantages of this tool.\n",
    "\n",
    "It is possible to come across classes that are not directly serializable with\n",
    "dill (e.g., keras models). In such cases, override methods 'to_pickleable_obj'\n",
    "and 'from_pickleable_obj' of class Processing method.\n",
    "\n",
    "Execute the next cell to save and load the previous algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% cd\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time lda:fit: 0.0024 s\n",
      "Result of lda_predict: [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Save algorithm\n",
    "alg.save('alg.pkl')\n",
    "\n",
    "# Load algorithm\n",
    "loaded_alg = components.Algorithm.load('alg.pkl')\n",
    "test_res = loaded_alg.exec_pipeline('test', x=x_test)\n",
    "\n",
    "print()\n",
    "print('Execution time lda:fit: %.4f s' % train_res[2]['perf']['run_time'])\n",
    "print('Result of lda_predict: %s' % str(test_res[2]['res']['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standalone algorithms\n",
    "\n",
    "Congratulations! The file alg.pkl in your working directory contains a\n",
    "standalone version of our mini-example. To load and use it in a different\n",
    "script or machine, use the following code:\n",
    "\n",
    "    >>> from medusa import components\n",
    "    >>> alg = components.Algorithm.load('alg.pkl')\n",
    "\n",
    "Standalone algorithms are very useful for developers and scientists that design\n",
    "add-hoc algorithms for a certain problem, database, etc, and want to share them\n",
    "in an easy and quick way.\n",
    "\n",
    "Remember that only algorithms that contain methods accessible in the destination\n",
    "machine can be distributed as a single file.\n",
    "For example, our tiny example can only be loaded in python environments which\n",
    "have sklearn installed. This shouldn't be a problem, even for the most\n",
    "complex examples, due to the huge amount of data processing packages available\n",
    "nowadays. Additionally, note that dill is able to deserialize functions from\n",
    "scratch, which means that you don't need the code of our little function\n",
    "'scale.'\n",
    "\n",
    "In the rare case that the available packages and dill functionalities don't suit\n",
    "your needs, you have 2 options to distribute your algorithm: distribute your\n",
    "code along with the algorithm file or create your own package in PyPI to easily\n",
    "install your methods in any computer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "That's all for now! Now you have a comprehensive picture of the functions and\n",
    "classes involved in the creation of medusa algorithms. As you can see, you can\n",
    "build full signal processing pipelines in a very flexible and easy way with\n",
    "few code lines using Medusa!\n",
    "\n",
    "See you in the next tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}